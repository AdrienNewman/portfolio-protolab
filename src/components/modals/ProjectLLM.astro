---
---
<div id="project-llm" class="modal-overlay">
    <div class="modal-content modal-large">
        <div class="modal-header">
            <h2 class="modal-title">
                <div class="modal-title-icon modal-title-icon-magenta"><svg viewBox="0 0 24 24"><circle cx="12" cy="12" r="3"/><path d="M12 1v6M12 17v6M4.22 4.22l4.24 4.24M15.54 15.54l4.24 4.24M1 12h6M17 12h6"/></svg></div>
                LLM Local + GPU
            </h2>
            <button class="modal-close" aria-label="Fermer"><svg viewBox="0 0 24 24"><path d="M18 6L6 18M6 6l12 12"/></svg></button>
        </div>
        <div class="modal-body">
            <div class="modal-section">
                <h4 class="modal-section-title">Présentation</h4>
                <p class="modal-text">Déploiement d'une infrastructure IA locale permettant d'exécuter des modèles de langage (LLM) avec accélération GPU. Ce projet combine virtualisation avancée et technologies d'intelligence artificielle.</p>
            </div>
            <div class="modal-section">
                <h4 class="modal-section-title">Architecture Technique</h4>
                <ul class="modal-list">
                    <li><strong>GPU Passthrough :</strong> NVIDIA RTX 3060 12GB dédiée via IOMMU/VFIO</li>
                    <li><strong>Runtime :</strong> Ollama pour l'exécution des modèles LLM</li>
                    <li><strong>Interface :</strong> Open WebUI pour l'interaction utilisateur</li>
                    <li><strong>Routing :</strong> LiteLLM pour le load-balancing multi-modèles</li>
                    <li><strong>Conteneurisation :</strong> Docker avec nvidia-container-toolkit</li>
                </ul>
            </div>
            <div class="modal-section">
                <h4 class="modal-section-title">Modèles Déployés</h4>
                <ul class="modal-list">
                    <li><strong>Llama 3.1 8B :</strong> Modèle généraliste pour conversations</li>
                    <li><strong>CodeLlama 7B :</strong> Assistance au développement</li>
                    <li><strong>Mistral 7B :</strong> Alternative performante</li>
                    <li><strong>Phi-3 :</strong> Modèle compact pour inférence rapide</li>
                </ul>
            </div>
            <div class="modal-section">
                <h4 class="modal-section-title">Défis Techniques Résolus</h4>
                <ul class="modal-list">
                    <li>Configuration IOMMU et groupes de GPU dans Proxmox</li>
                    <li>Installation drivers NVIDIA dans environnement virtualisé</li>
                    <li>Optimisation mémoire VRAM pour modèles volumineux</li>
                    <li>Sécurisation de l'accès via reverse-proxy authentifié</li>
                </ul>
            </div>
            <div class="modal-section">
                <h4 class="modal-section-title">Stack Technique</h4>
                <div class="modal-tags">
                    <span class="modal-tag">Ollama</span>
                    <span class="modal-tag">Open WebUI</span>
                    <span class="modal-tag">LiteLLM</span>
                    <span class="modal-tag">CUDA 12</span>
                    <span class="modal-tag">Docker</span>
                    <span class="modal-tag">NVIDIA Container Toolkit</span>
                    <span class="modal-tag">Proxmox GPU Passthrough</span>
                </div>
            </div>
        </div>
    </div>
</div>
